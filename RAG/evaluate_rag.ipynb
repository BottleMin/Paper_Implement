{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOu4+D4Vqg9nbaiq4tv5oLr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BottleMin/Paper_Implement/blob/main/RAG/evaluate_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "y_7iKT0_cfme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "%cd /content/drive/MyDrive/rag_project\n",
        "\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu\n",
        "!pip install transformers datasets\n"
      ],
      "metadata": {
        "id": "PVUms0NHJD2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import faiss\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
        "from transformers import logging as transformers_logging\n",
        "from datasets import Dataset, load_from_disk\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logging.set_verbosity_info()"
      ],
      "metadata": {
        "id": "eqGvfW36ZF6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 정규화 및 평가 함수 정의\n",
        "\n",
        "- `normalize_answer(s)`: 문자열을 정규화하는 함수. 소문자 변환, 구두점 제거, 불필요한 공백 제거 등을 수행.\n",
        "\n",
        "- `f1_score(prediction, ground_truth)`: 예측된 답변과 실제 답변 간의 F1 점수.\n",
        "\n",
        "- `exact_match_score(prediction, ground_truth)`: 예측된 답변과 실제 답변이 정확히 일치하는지 확인."
      ],
      "metadata": {
        "id": "BZ7f-I4NR9vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import Callable, Dict, Iterable, List\n",
        "\n",
        "def normalize_answer(s):\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return normalize_answer(prediction) == normalize_answer(ground_truth)"
      ],
      "metadata": {
        "id": "U82XeDD9Kf9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 평가 함수\n",
        "\n",
        "- `evaluate_batch_e2e(args, rag_model, questions)`: 모델을 사용하여 질문에 대한 답변을 생성하는 함수.\n",
        "\n",
        "질문을 토큰화하고 입력 데이터로 준비. 모델을 통해 답변을 생성하고, 생성된 답변을 디코딩한다.\n",
        "\n",
        "- `args.print_predictions`가 참이면 질문과 답변을 로그에 기록한다."
      ],
      "metadata": {
        "id": "_sWJXqX6SQZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_batch_e2e(args, rag_model, questions):\n",
        "    with torch.no_grad():\n",
        "        # Tokenize the questions\n",
        "        inputs_dict = rag_model.retriever.question_encoder_tokenizer.batch_encode_plus(\n",
        "            questions, return_tensors=\"pt\", padding=True, truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids = inputs_dict['input_ids'].to(args.device)\n",
        "        attention_mask = inputs_dict['attention_mask'].to(args.device)\n",
        "\n",
        "        outputs = rag_model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            num_beams=args.num_beams,\n",
        "            min_length=args.min_length,\n",
        "            max_length=args.max_length,\n",
        "            early_stopping=True,\n",
        "            num_return_sequences=1,\n",
        "            bad_words_ids=[[0, 0]],  # BART likes to repeat BOS tokens, dont allow it to generate more than one\n",
        "        )\n",
        "        answers = rag_model.retriever.generator_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        if args.print_predictions:\n",
        "            for q, a in zip(questions, answers):\n",
        "                logger.info(\"Q: {} - A: {}\".format(q, a))\n",
        "\n",
        "        return answers"
      ],
      "metadata": {
        "id": "NnIiet_DZc7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description=\"Test Script\")\n",
        "\n",
        "parser.add_argument(\"--model_name_or_path\", type=str, default=\"/content/drive/MyDrive/rag_project/trained_model\", help=\"Path to pretrained model or model identifier\")\n",
        "parser.add_argument(\"--evaluation_set\", type=str, default=\"nq-test.csv\", help=\"Path to the evaluation dataset (CSV file)\")\n",
        "parser.add_argument(\"--predictions_path\", type=str, default=\"predictions.txt\", help=\"Path to save predictions\")\n",
        "parser.add_argument(\"--num_beams\", type=int, default=4, help=\"Number of beams for generation\")\n",
        "parser.add_argument(\"--min_length\", type=int, default=1, help=\"Minimum length of generated answers\")\n",
        "parser.add_argument(\"--max_length\", type=int, default=50, help=\"Maximum length of generated answers\")\n",
        "parser.add_argument(\"--eval_batch_size\", type=int, default=8, help=\"Batch size for evaluation\")\n",
        "parser.add_argument(\"--print_predictions\", action=\"store_true\", help=\"If true, print predictions\")  # 추가된 부분\n",
        "\n",
        "args = parser.parse_args([])\n",
        "\n",
        "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "loaded_faiss_index = faiss.read_index('embeddings.faiss')\n",
        "\n",
        "loaded_embeddings_dataset = Dataset.load_from_disk('wiki_dataset_without_indexes')\n",
        "\n",
        "# 로드된 FAISS 인덱스를 데이터셋에 추가\n",
        "loaded_embeddings_dataset.add_faiss_index(column='embeddings')\n",
        "\n",
        "\n",
        "retriever = RagRetriever.from_pretrained(args.model_name_or_path, indexed_dataset = loaded_embeddings_dataset)\n",
        "model = RagSequenceForGeneration.from_pretrained(args.model_name_or_path, retriever=retriever)\n",
        "model.to(args.device)"
      ],
      "metadata": {
        "id": "-AQwZuqsJ95q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"***** Running evaluation *****\")\n",
        "logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "logger.info(\"  Predictions will be stored under %s\", args.predictions_path)\n",
        "\n",
        "import gc\n",
        "import random\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "random_batch = random.randrange(len(question))\n",
        "\n",
        "# 하나의 질문에 대해서만 예측 수행\n",
        "eval_data = pd.read_csv(args.evaluation_set)\n",
        "question = eval_data['Question'].tolist()\n",
        "answers = eval_data['Answer'].tolist()\n",
        "\n",
        "batch_question = [question[random_batch]]\n",
        "gold_answer = answers[random_batch]\n",
        "print(f\"Evaluating for question: {batch_question}\")\n",
        "\n",
        "# 예측 수행\n",
        "\n",
        "\n",
        "batch_answers = evaluate_batch_e2e(args, model, batch_question)\n",
        "\n",
        "# 예측 결과 출력\n",
        "predicted_answer = batch_answers[0]\n",
        "print(f\"Predicted answer: {predicted_answer}\")\n",
        "print(f'Gold answer: {gold_answer}')"
      ],
      "metadata": {
        "id": "x7hSSBNwNHfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " f1_score = f1_score(predicted_answer, gold_answer)\n",
        " print(f'f1 score : {f1_score}')"
      ],
      "metadata": {
        "id": "3Chd9f_NuoBk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}