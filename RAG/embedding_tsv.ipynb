{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMkbJ5rHD+ay3X25Yr8XRTV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uMik3cbZsmZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/rag_project\n",
        "\n",
        "!pip install transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "pbMfAA3-svsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "\n",
        "tsv_file = \"psgs_w100.tsv\"\n",
        "dataset_dict = datasets.load_dataset('csv', data_files=tsv_file, delimiter='\\t')\n",
        "\n",
        "# 데이터셋 변환\n",
        "dataset = dataset_dict['train']\n",
        "\n",
        "# 사전 훈련된 DPR 모델 및 토크나이저 로드\n",
        "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
        "ctx_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
        "\n",
        "# 장치 설정 (GPU가 사용 가능한 경우 GPU 사용)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "ctx_encoder.to(device)\n",
        "\n",
        "# 텍스트 임베딩 함수\n",
        "def compute_embeddings(batch, ctx_tokenizer, ctx_encoder):\n",
        "    inputs = ctx_tokenizer(batch['text'], truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        embeddings = ctx_encoder(**inputs).pooler_output\n",
        "    return {'embeddings': embeddings.cpu().numpy().tolist()}\n",
        "\n",
        "embeddings_dataset = dataset.map(lambda batch: compute_embeddings(batch, ctx_tokenizer, ctx_encoder), batched=True, batch_size=128)\n"
      ],
      "metadata": {
        "id": "oTvN8azuHCMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = embeddings_dataset.list_indexes()\n",
        "for index in indexes:\n",
        "    embeddings_dataset.drop_index(index)\n",
        "\n",
        "embeddings_dataset.save_to_disk('/content/drive/MyDrive/rag_project/wiki_dataset_without_indexes')\n",
        "\n",
        "embeddings_dataset.add_faiss_index(column='embeddings')\n",
        "\n",
        "faiss.write_index(embeddings_dataset.get_index('embeddings').faiss_index, '/content/drive/MyDrive/rag_project/embeddings.faiss')\n",
        "\n",
        "print(\"임베딩 및 FAISS 인덱스가 저장되었습니다.\")\n",
        "\n",
        "loaded_embeddings_dataset = datasets.Dataset.load_from_disk('/content/drive/MyDrive/rag_project/embedded_dataset')\n",
        "loaded_faiss_index = faiss.read_index('/content/drive/MyDrive/rag_project/embeddings.faiss')\n",
        "\n",
        "print(\"데이터셋의 임베딩 수:\", len(loaded_embeddings_dataset))\n",
        "print(\"Faiss 인덱스의 임베딩 수:\", loaded_faiss_index.ntotal)"
      ],
      "metadata": {
        "id": "X4qZIS5xHHW9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}